\chapter{数据处理的概率论基础}

前面的章节我们介绍了最基本的数据模型——线性模型。
从中可以发现线性回归的关键在于处理数据中的噪音和误差；没有噪音和误差数据和模型应该完美吻合，不需要线性回归求近似解。噪音和误差的本质都是不确定性，而量化不确定性最有效的数学工具是概率论，因此本章我们补充一些数据处理的概率论基础。

\section{什么是概率}
概率论的研究对象自然是\emph{概率}。
概率的基本含义很容易理解，概率是一个$0$到$1$之间的实数，代表某件事情发生的可能性——数字越大代表事件发生的可能性越大，两个极端分别是概率$0$，事情绝对不会发生，概率$1$，事情一定会发生。
但要进行严密的数学分析需要对概率的意义做严格的定义，这就没有那么符合直觉了。例如概率$0.85$代表什么意思？

关于概率的定义历史上有过很长时间的争论，一直到今天都存在两种互相竞争的定义，一种是基于\emph{频率}的概率，简称\emph{频率概率}，另一种是基于\emph{贝叶斯公式}的概率，简称\emph{贝叶斯概率}。用一句话总结
\begin{itemize}
    \item 前者认为概率是重复实验中频率的极限；
    \item 后者认为概率是一个用贝叶斯公式反复修正后的数字。
\end{itemize}
两种概率定义在很多应用中是等价的，但是两者背后隐含了非常不同的哲学思考，在另一些应用中会得到不同的结果。

\subsection{频率概率}

频率的定义来自于重复实验。假设某种试验一共有$M$种可能的结果，分别用自然数$i\in\{1,2,\cdots, M\}$表示。将试验重复进行$N$次，统计后结果为$i$的次数记为$n_i$，$i$出现的\emph{频率（frequency）}记为
\begin{equation}
    f_i=\frac{N_i}{N}。
\end{equation}

由于每次试验的结果随机，因此试验总次数$N$不同时频率$f_i$也会变化。但是概率论的一个基本假设是，随着试验重复次数$N$趋于无穷大，频率$f_i$会收敛于一个确定值这个值就是结果$i$的\emph{概率（probability）}
\begin{equation}
    p_i=\lim_{N\to\infty}f_i=\lim_{N\to\infty}\frac{n_i}{N}。
\end{equation}


凭率概率最大的优势是它的定义建立在重复试验上，具有\emph{客观性}，因为无论是重复试验还是结果计数都与实验者的主观意愿无关。
由于客观性这个优势，频率概率在很长时间内占据了概率论研究的主流。

频率概率的最大问题也来自于重复试验，因为在实际应用中会遇到很多数据背后的试验无法重复的情况。
例如天文学数据来自对天文现象的观测，而很多天文想象的周期远超过天文学家的寿命，甚至超过人类历史的长度；
或者安全事故数据来自于飞机轮船灾难，我们既不知道灾难以后什么时候会发生，也不希望它发生，更不能主动安排它发生。对于这些情况贝叶斯概率更加适合。

\subsection{贝叶斯概率}
贝叶斯概率来自于\emph{贝叶斯贝叶斯公式}\sidenote{公式以发现者托马斯·贝叶斯牧师（Thomas Bayes）的名字命名}
\begin{equation}\label{eq:bayes}
    P(A|B)=\frac{P(B|A)P(A)}{P(B)}
\end{equation}
其中$A$、$B$是两个不同的事件；$P(\cdot)$是概率函数，以事件为输入，输出$0$到$1$之间的数字表示该事件发生的可能性。

\subsubsection{推导}
贝叶斯公式来自于更基础的\emph{条件概率公式}
\begin{equation}\label{eq:conditional1}
    P(AB)=P(A)P(B|A)。
\end{equation}
即$A$、$B$两个事件同时发生的概率$P(AB)$，等于$A$事件单独发生的概率$P(A)$乘以$A$事件发生的条件下$B$事件发生的概率$P(B|A)$。
同时我们注意到\cref{eq:conditional1}中$A$、$B$是对称的，因此可以交换两个符号得到
\begin{equation}\label{eq:conditional2}
    P(BA)=P(B)P(A|B)。
\end{equation}

根据定义，$A$、$B$同时发生的概率应该等于$B$、$A$同时发生的概率，因此我们有
\begin{align}
    P(BA)&=P(AB)，\\
    \intertext{代入\cref{eq:conditional1}和\cref{eq:conditional2}得到}
    P(B)P(A|B) &= P(A)P(B|A)。
\end{align}
两边同时除以$P(B)$就能得到\cref{eq:bayes}。

\subsubsection{解读}
为了方便理解贝叶斯公式的重要意义，我们把\cref{eq:bayes}中的符号$A$、$B$替换为$H$、$D$，其中$H$表示\emph{假说（Hypothesis）}，$D$表示\emph{数据（Data）}，由此贝叶斯公式写作
\begin{equation}\label{eq:bayes-hd}
    P(H|D)=\frac{P(D|H)P(H)}{P(D)}。
\end{equation}
此时公式中的四个组成部分都有了实际意义，其中：
\begin{itemize}
    \item $P(H)$称为\emph{先验概率（prior）}，表示在不考虑任何数据的情况下，分析者对某假说$H$成立可能性的\emph{主观}评价；
    \item $P(H|D)$称为\emph{后验概率（posterior）}，表示在考虑数据后，分析者对假说成立可能性的评价；
    \item $P(D|H)$称为\emph{似然概率（likelihood）}，表示如果假说$H$成立，观察到数据$D$的可能性；
    \item $P(D)$称为\emph{全数据概率（evidence）}，表示综合考虑假说$H$是否成立后，观察到数据$D$的概率。
\end{itemize}

注意，在贝叶斯概率的定义里并没有告诉我们如何去确定先验概率和似然概率的值%
\sidenote{全数据概率$P(D)$通常被忽略，见\cref{eq:bayes-propto}。}
，只是告诉我们假设两个概率确定后如何用似然概率去修正先验概率得到后验概率。

\section{贝叶斯估计}

贝叶斯公式的重要意义在于提供了一种工具，让我们从主观判断出发，利用数据对我们的判断进行修正，得到更接近于事实的判断。
例如对于某个问题我们知道一共有三种可能的答案$A$、$B$、$C$，但不确定正确答案是哪一个。
我们可以把这个不确定的答案称为\emph{随机变量}$X$，由此得到三个不同的假设
\begin{equation*}
    H_A:X=A,\qquad H_B:X=B,\qquad H_C:X=C。
\end{equation*}

我们从主观判断出发，可以随意给定三个假说的先验概率$P(H_A)$、$P(H_B)$、$P(H_C)$，唯一的要求是三者之和为$1$，即$P(H_A)+P(H_B)+P(H_C)=1$，因为三个假说必有一个成立。
接下来为了利用贝叶斯公式我们需要采用某种方法获取似然概率$P(D|H_A)$、$P(D|H_B)$、$P(D|H_C)$，但另一方面全数据概率却$P(D)$可以直接确定。因为三个后验概率之和也要为$1$，即
\begin{equation*}
    P(H_A|D)+P(H_B|D)+P(H_C|D)=1，
\end{equation*}
带入\cref{eq:bayes-hd}得到
\begin{equation}
    \frac{P(D|H_A)P(H_A)}{P(D)}+\frac{P(D|H_B)P(H_B)}{P(D)}+\frac{P(D|H_C)P(H_C)}{P(D)}=1。
\end{equation}
可以看出$P(D)$只是保证后验概率之和为$1$的\emph{归一化（normalization）}系数，一旦先验概率和似然概率确定，它的值也自动确定。

因此我们常常省去全数据概率$P(D)$，将\cref{eq:bayes-hd}简写成
\begin{equation}\label{eq:bayes-propto}
    P(H|D)\propto P(D|H)P(H)，
\end{equation}
其中$\propto$读作“正比于”。
贝叶斯公式读作“后验概率正比于先验概率与似然概率的乘积”。

贝叶斯估计的思想在现实生活中广泛存在。在《血字的研究》第二章中，福尔摩斯通过推理确定华生来自阿富汗时就不自觉的使用了贝叶斯估计的原理：
\begin{quotation}
    “在你这件事上，我的推理过程是这样的：‘这一位先生，具有医务工作者的风度，但却是一副军人气概。那么，显见他是个军医。他是刚从热带回来，因为他脸色黝黑，但是，从他手腕的皮肤黑白分明看来，这并不是他原来的肤色。他面容憔悴，这就清楚地说明他是久病初愈而又历尽了艰苦。他左臂受过伤，现在动作品来还有些僵硬不便。\emph{试问，一个英国的军医在热带地方历尽艰苦，并且臂部负过伤，这能在什么地方呢？自然只有在阿富汗了。}’这一连串的思想，历时不到一秒钟，因此我便脱口说出你是从阿富汗来的，你当时还感到惊破哩。”
\end{quotation}

