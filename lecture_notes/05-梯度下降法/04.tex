%\documentclass[14pt,aspectratio=169]{beamer}
\documentclass[14pt]{beamer}
\usepackage{ctex}
\usepackage{bm}
\usepackage{color, colortbl}
\definecolor{HRed}{rgb}{1,.2,.2}
%\usepackage{xeCJK} % important! Without this Chinese fonts won't show
%\usepackage{xeCJKfntef}
\setsansfont{Noto Sans CJK SC Light}
\setCJKsansfont[ItalicFont=Kaiti SC]{Noto Sans CJK SC Light}
\setCJKmainfont{Noto Serif CJK SC Light}

\usefonttheme[onlymath]{serif} % formulars in serif font
\usefonttheme{professionalfonts} % 防止公式间距异常，参见https://www.zhihu.com/question/55492768
\parskip=10pt

%\newcommand{\mat}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{#1}}
\renewcommand{\vec}[1]{\bm{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\MA}{\mat{A}}
\newcommand{\MI}{\mat{I}}
\newcommand{\Va}{\Vec{a}}
\newcommand{\Vy}{\vec{y}}
\newcommand{\Vx}{\vec{x}}
\newcommand{\Ve}{\vec{e}}
\newcommand{\Vt}{\vec{\theta}}
\newcommand{\SR}{\mathcal{R}}

\let\emph\relax % there's no \RedeclareTextFontCommand
\DeclareTextFontCommand{\emph}{\color{red}\em}
\setbeamertemplate{headline}{}
\setbeamertemplate{navigation symbols}{}

%%%%%%%%%%%%%%% Section标题页 %%%%%%%%%%%%%%%%%%%%%%%
\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
%%%%%%%%%%%%%%% 正文开始 %%%%%%%%%%%%%%%%%%%%%%%

\title{第4讲：梯度下降法}
\subtitle{线性回归的数值解}
\author{熊耀华}
\institute{交通工程系}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
  \frametitle{避免过拟合}
  测试如果发现模型过拟合，说明模型的自由度过大，解决方法有
  \begin{itemize}
      \item 减少参数数量
      \item 约束参数取值范围
  \end{itemize}
  第二种方法称为\emph{正则化}（Regularization）。
\end{frame}

\begin{frame}
  \frametitle{Ridge Regression}
  线性回归有多种正则化形式，最常见的可能是Ridge Regression。损失函数定义为
  \begin{equation}
      L(\Vt)=\Ve^T\Ve+{\color{red}\frac{1}{2}\lambda\Vt^T\Vt}
  \end{equation}
  其中$\Vt^T\Vt$是所有参数$\Vt$的平方和，$\lambda$是\emph{超参数}
  表示参数平方和的相对重要性

  这个损失函数要求在残差平方和$\Ve^T\Ve$最小的基础上让参数的平方和也尽量小。
\end{frame}

\begin{frame}
  \frametitle{超参数和参数}
  $n$次多项式回归可以写成
  \[\Vy=f(\Vx,\Vt,n)\]
  Ridge回归可以写成
  \[\Vy=f(\Vx,\Vt,\lambda)\]
  其中$n$和$\lambda$都是\emph{超参数}。
  
  超参数与参数的区别在于，超参数在训练前确定，不受训练过程影响。
  而参数随训练过程改变。
\end{frame}

\begin{frame}
  \frametitle{Ridge Regression解析解}
  使用同样的分析方法，求解偏导数等于$\vec{0}$的方程
  \begin{equation}
      \frac{\partial L(\Vt)}{\partial\Vt}\bigg|_{\Vt^*}=\vec{0}
  \end{equation}
  可以得到最优参数的解析解
  \begin{equation}
      (\MA^T\MA+{\color{red}\lambda\MI})\Vt=\MA^T\Vy
  \end{equation}

  $\lambda\MI$抬高了$\MA^T\MA$矩阵对角线上的值，看起来像\emph{山脊}（Ridge）
\end{frame}

\begin{frame}
  \frametitle{练习}
  使用大小不同的$\lambda$拟合数据，总结规律。
\end{frame}

\begin{frame}
  \frametitle{Ridge Regression的优势}
  给矩阵$\MA^A$添加“山脊”$\lambda\MI$有两方面的好处
  \begin{itemize}
    \item 一方面约束了$\Vt$的取值范围，让函数平滑，避免高频振荡带来的过拟合
    \item 另一方面增加了方程组的\emph{数值稳定性}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{线性方程的数值稳定性}
  将线性方程组$\MA\Vx=\Vy$右侧增加\emph{扰动项}$\Delta\Vy$，为了等式成立
  左侧应相应变化某个$\Delta\Vx$，即
  \begin{equation}
    A(\Vx+\Delta\Vx)=\Vy+\Delta\Vy
  \end{equation}
  当$\|\Delta\Vy\|$很小时$\|\Delta\Vx\|$也很小，
  我们说方程\emph{稳定}；反之不稳定
\end{frame}

\begin{frame}
  \frametitle{练习}
  我们有\[\MA=\begin{bmatrix}
    10 & 0\\
    0 & 0.1
  \end{bmatrix}\qquad
  \lambda = 5
  \]
  分别探讨$\MA$和$\MA+\lambda\MI$的稳定性
\end{frame}

\begin{frame}
  \frametitle{为什么要分析稳定性？}
  理想状态下每一步计算数据完全准确，稳定性分析没有意义。但是实际计算中
  每一步都会引入误差，例如
  \begin{itemize}
    \item 自变量$\Vx$的测量不能完全避免误差
    \item 手工计算中只保留小数点后3位
    \item 计算机浮点数精度有限，很多数字只能\emph{近似}表示
  \end{itemize}

  此时如果方程不稳定，求解过程误差\emph{放大}严重，最终结果没有意义。
\end{frame}

\section{线性回归数值解}
\begin{frame}
  \frametitle{解析解}
  之前的例子通过公式推导求解
  $$\Vt^*=\argmin L(\Vt)$$
  得到的公式称为\emph{解析解}，例如
  \begin{equation*}
    \MA^T\MA\Vt^*=\MA^T\Vy \qquad (\MA^T\MA+\lambda\MI)\Vt^*=\MA^T\Vy
  \end{equation*}
\end{frame}

\begin{frame}
  \frametitle{数值解}
  解析解虽然在数学上更优雅，但实际应用中往往通过\emph{数值方法}求\emph{数值解}
  \begin{itemize}
    \item 很多模型的损失函数没有解析解，只能求数值解
    \item 对于大量数据，数值解可以用\emph{精度换时间}，快速得到低精度结果
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{梯度下降法}
  \emph{梯度下降法}（Gradient Descent）是最小化问题中的常用数值方法，基本思路如下
  \begin{enumerate}
    \item 将目标函数$L(\Vt)$看作起伏不平的地面，要找到地面最低点
    \item 从随机位置开始
    \item 观察该位置上不同方向的坡度，选择坡度\emph{下降最快}的方向，前进一定距离
    \item 新位置坡度是否为$0$？
    \item 是则到达最低点，结束；否则回到第3步
  \end{enumerate}
\end{frame}
  
\begin{frame}
  \frametitle{梯度方向}
  连续函数$L(\Vt)$的\emph{梯度}$\nabla L$（读做nabla L）是所有偏导数构成的向量
  \begin{equation}
    \nabla_{\Vt} L(\Vt) = \begin{bmatrix}
      \frac{\partial L}{\partial\theta_1}(\Vt)\\
      \frac{\partial L}{\partial\theta_2}(\Vt)\\
      \frac{\partial L}{\partial\theta_3}(\Vt)\\
      \vdots
    \end{bmatrix}
  \end{equation}
  用梯度可以估计函数自变量变化$\Delta\Vt$时，函数值的变化
  $$\Delta L ={{\nabla L(\Vt)}^T\cdot\Delta\Vt}$$
\end{frame}

\begin{frame}
  \frametitle{梯度反方向下降最快}
  可以证明，如果$\Delta\Vt$长度一定，方向可变，那么当$\Delta\Vt$与梯度方向一致时$\Delta L$最大。
  
  提示：向量点积有如下性质
  $$\Vx\cdot\Vy=\|\Vx\|\|\Vy\|\cos\alpha$$
  其中$\alpha$是两个向量的夹角。

  因此梯度方向是函数值上升最快的方向，而梯度的反方向是函数值\emph{下降最快}的方向。
\end{frame}

\begin{frame}
  \frametitle{梯度下降迭代公式}
  基于以上分析，梯度下降法可以表示为迭代公式
  \begin{equation}
    \Vt^{n+1}=\Vt^n-\eta\cdot\nabla_{\Vt} L(\Vt^n)
  \end{equation}
  其中$\eta$（eta）称为\emph{学习速率}（Learning Rate），控制参数$\Vt$的迭代更新速度
\end{frame}

\begin{frame}
  \frametitle{线性回归的梯度}
      线性回归的损失函数是
      \begin{equation*}
        L(\Vt)=\frac{1}{m}(\MA\Vt-\Vy)^T(\MA\Vt-Vy)
      \end{equation*}
      对$\Vt$求偏导有
      \begin{equation}\label{eq:gradient_ls}
        \begin{split}
        \nabla_{\Vt}L(\Vt)&=\frac{1}{m}[\MA^T\MA\Vt-\MA^T\Vy]\\
        &=\frac{1}{m}\MA^T(\MA\Vt-\Vy)\\
        &=\boxed{\frac{1}{m}\MA^T(\hat{\Vy}-\Vy)}
        \end{split}
      \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Ridge回归的梯度}
      Ridge回归的损失函数是
      \begin{equation*}
        L(\Vt)=\frac{1}{m}(\MA\Vt-\Vy)^T(\MA\Vt-Vy)+\lambda\Vt^T\Vt
      \end{equation*}
      对$\Vt$求偏导有
      \begin{equation}
        \begin{split}
        \nabla_{\Vt}L(\Vt)&=(\frac{1}{m}\MA^T\MA+\lambda\MI)\Vt-\frac{1}{m}\MA^T\Vy\\
        &=\boxed{\frac{1}{m}\MA^T(\hat{\Vy}-\Vy)+\color{red}{\lambda\Vt^T\Vt}}
        \end{split}
      \end{equation}
      
\end{frame}

\begin{frame}
  \frametitle{练习}
  使用梯度下降法求解线性回归，画出损失函数和迭代次数的变化曲线，也称为\emph{学习曲线}
\end{frame}

\begin{frame}
  \frametitle{梯度下降法常见问题}
  使用梯度下降法容易遇到的问题有
  \begin{itemize}
    \item 学习速率过大，\emph{震荡}而不收敛
    \item 学习速率过小，收敛慢
    \item 遇到\emph{局部最优值}，落入“陷阱”
    \item 遇到\emph{平台}，收敛慢
    \item 梯度计算\emph{太复杂}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{改进梯度计算}
  之前的梯度计算基于总损失函数，是\emph{所有数据点}残差平方之和
  \begin{equation*}
    \begin{split}
    L(\Vt)&=\frac{1}{m}\sum_{i=1}^{m}(ax_i+b-y_i)^2\\
    &=\frac{1}{m}((ax_1+b-y_1)^2+(ax_2+b-y_2)^2+\cdots)\\
    &=\frac{1}{m}(l_1(\Vt)+l_2(\Vt)+\cdots)
    \end{split}
  \end{equation*}
  计算梯度需要计算每一项的偏导，因此称为\emph{批处理}（batch）。
\end{frame}

\begin{frame}
  \frametitle{改进梯度计算} 
  当数据量很大时，批处理计算速度较慢，需要优化。显然的思路是只计算
  \emph{部分数据}残差的偏导
  \begin{itemize}
    \item 随机（stochastic）梯度下降，每次迭代随机选取\emph{一个}数据计算下降方向
    \item 分批（mini-batch）梯度下降，每次迭代随机选取\emph{一组}数据计算下降方向
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{随机梯度下降}
  以线性回归为例，第$k$次迭代中随机选择了第$i(k)$个数据$(x_{i(k)}, y_{i(k)})$。此时损失函数为
  \begin{equation}
    L(\Vt)=l_{i(k)}(\Vt)=(ax_{i(k)}+b-y_{i(k)})^2
  \end{equation}
  对$\Vt$求偏导有
  \begin{equation}
      \nabla_{\Vt}L(\Vt) =\begin{bmatrix}
        x_{i(k)}\\
        1
      \end{bmatrix}(\begin{bmatrix}
        x_{i(k)} & 1
      \end{bmatrix}\begin{bmatrix}
        a\\
        b
      \end{bmatrix}-y_{i(x)})
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{分批梯度下降}
  分批（mini-batch）梯度下降是batch和stochastic之间的妥协。
  
  某次迭代中随机选择一组$n$个数据，损失函数为TODO
  \begin{equation}
    L(\Vt)=\frac{1}{n}\sum_{i=1}^{n}l_i(\Vt)
  \end{equation}
  其中$\MA^{(n)}$是$\MA$的随机选择$n$行构成的\emph{子矩阵}（sub matrix），偏导为
  \begin{equation}
      \nabla_{\Vt}L(\Vt)= \MA{^{(n)}}^T(\MA^{(n)}\Vt-\Vy^{(n)})
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{练习}
  用stochastic和mini-batch方法求解线性回归，画出$\Vt$的\emph{相空间}图。
\end{frame}

\begin{frame}
  \frametitle{Lasso回归}
 Least Absolute Shrinkage and Selection Operator Regression，简称\emph{Lasso}回归。
 
 Ridge回归的损失函数是
 $$L(\Vt)=\Ve^T\Ve+\lambda\Vt^t\Vt=\Ve^T\Ve+\lambda{\color{red}\sum\theta_i^2}$$
而Lasso回归的损失函数是
\begin{equation}
L(\Vt)=\Ve^T\Ve+\lambda{\color{red}\sum|\theta_i|}
\end{equation}
\end{frame}

\begin{frame}
  \frametitle{Ridge、Lasso和范数}
  Ridge回归的\emph{正则项}是参数矢量$\Vt$各元素的\emph{平方和}，Lasso回归中的正则项是元素的\emph{绝对值之和}。

  本质上是两种计算矢量“长度”的方法，是矢量\emph{范数}（norm）的特殊形式。对于矢量$\Vx={x_1,\ldots,x_n}$范数的一般定义为
  \begin{equation}
    \|\Vx\|_p=\left(\sum_{i=1}^n{|x_i|}^p\right)^{1/p}
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{常用范数}
  范数$\|\|_p$的参数$p$可以取任何正数。但常见的取值和他们的性质如下
  \begin{description}
    \item[$\|\Vx\|^0$] 矢量中非0元素的个数，严格来说不是范数
    \item[$\|\Vx\|^1$] 绝对值之和，带来稀疏性
    \item[$\|\Vx\|^2$] 平方和，又称\emph{欧几里德长度}，符合我们的几何直觉
    \item[$\|\Vx\|^\infty$] 最大元素$\max\{x_1,\ldots,x_n\}$
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Lasso带来参数稀疏性}
  Lasso回归使用$1$次范数作为正则项，目的是使参数矢量$\Vt$\emph{稀疏}。

  $1$次正则项的几何形状可以解释稀疏性
  
  稀疏的好处是，大量参数为$0$，对应的属性对模型预测不产生影响。因此通过Lasso回归可以识别\emph{相关属性}和\emph{无关属性}。
\end{frame}

\begin{frame}
  \frametitle{Ridge和Lasso的混合}
  Ridge回归具有\emph{稳定性}而Lasso回归具有\emph{稀疏性}。Elastic net是两者混合，损失函数定义为
  \begin{equation}
    L(\Vt)=\Ve^T\Ve+\eta(r\sum|\theta_i|+(1-r)\frac{1}{2}\sum\theta_i^2)
  \end{equation}
  其中$r$表示Ridge和Lasso的相对权重。

  适当的混合比例可以使模型同时具有两者的良好性质。
\end{frame}

\section{分类模型}

\begin{frame}
  \frametitle{分类模型}
  前面的回归模型中，我们建立了函数关系$f:\Vx\to\Vy$用于从$\Vx$预测$\Vy$，此时$\Vy\in \SR$可以取任何值。

  现在我们希望建立函数，判断$\Vx$\emph{是否}满足某种条件。此时$\Vy$应当表示满足条件的概率，$\Vy\in[0,1]$

  为此我们可以构造一个函数$g:\SR\to[0,1]$，与$f$组成复合函数
  \begin{equation}
    g\circ f:\Vx\to[0,1]
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Logistic函数}
  满足$g:\SR\to[0,1]$的函数中，最常用的称为Logisitic函数（或者Sigmoid函数）
  \begin{equation}
    \sigma(t)=\frac{1}{1+e^{-t}}
  \end{equation}
  从函数图像可以看出
  \begin{itemize}
    \item $\lim_{t\to\infty}\sigma(t)=1$
    \item $\lim_{t\to-\infty}\sigma(t)=0$
    \item 关于$(0, 0.5)$对称
  \end{itemize}
  

\end{frame}
\begin{frame}
  \frametitle{Logistic回归}
  用以下复合函数拟合$\Vx$，$\Vy$
  \begin{equation}
    \sigma(\MA\Vt)
  \end{equation}
  其中，$\Vy$矢量中的元素只能取$\{0,1\}$，分别表示两种\emph{可能的类别}；$\MA=\begin{bmatrix}
    \Vx & \vec{1}
  \end{bmatrix}$

  通过选取最佳的参数$\Vt^*$使\emph{预测值}$\hat{\Vy}=\sigma(\MA\Vt^*)$与\emph{观测值}$\Vy$\emph{最接近}。如何定义“接近”？
\end{frame}

\begin{frame}
  \frametitle{Logistic函数的导数}
  梯度下降法求解时需要Logistic函数的导数。Logistic函数的导数具有\emph{特别形式}
  \begin{equation}
    \sigma'(t)=\sigma(t)(1-\sigma(t))
  \end{equation}
  请验证？
  
  提示：用求导规则
  $$\left(\frac{1}{u}\right)'=\frac{u'}{u^2}$$
\end{frame}
\begin{frame}
  \frametitle{如何比较概率？}
  线性回归中将接近定义为\emph{残差的平方和}，这个定义不适合Logistic回归。因为Logistic回归中的$\Vy$表示\emph{概率}，而概率的接近程度并不是线性的。
  
  考虑以下三种情况，交通事故发生的概率为$\{0.01, 0.001, 0.0001\}$。概率数字变化不大，但表示的情况差别很明显，三种情况的安全性各差了一个\emph{数量级}。

  要体现数量级的差距，用概率的\emph{对数函数}更合适。例如$\{\log0.01, \log0.001, \log0.0001\}$三个值之间距离相等。
\end{frame}
\begin{frame}
  \frametitle{避免过拟合}
  测试发现模型过拟合说明模型的自由度过大，解决方法有
  \begin{itemize}
      \item 减少参数数量
      \item 约束参数取值范围
  \end{itemize}
  第二种方法称为\emph{正则化}（Regularization）。
\end{frame}

\begin{frame}
  \frametitle{超参数和参数}
  多项式回归本质上是根据数据构造一个$n$次多项式函数
  \[\Vy=f(\Vx,\Vt,n)\]
  函数的自变量中包括
  \begin{itemize}
      \item 数据$\Vx$
      \item 参数$\Vt$
      \item \emph{超参数}(Hyper-parameter)$n$ 
  \end{itemize}
  超参数与参数的区别在于，超参数在训练前确定，不受训练过程影响。
  而参数随训练过程改变。
\end{frame}

\begin{frame}
  \frametitle{Ridge Regression}
  线性回归有多种正则化形式，最常见的可能是Ridge Regression。损失函数定义为
  \begin{equation}
      L(\Vt)=\Ve^T\Ve+{\color{red}\frac{1}{2}\lambda\Vt^T\Vt}
  \end{equation}
  其中$\Vt^T\Vt$是所有参数$\Vt$的平方和，$\lambda$是超参数
  表示参数平方和的相对重要性

  这个损失函数要求在残差平方和$\Ve^T\Ve$最小的基础上让参数的平方和也尽量小。
\end{frame}

\begin{frame}
  \frametitle{Ridge Regression解析解}
  使用同样的分析方法，求解偏导数等于$\vec{0}$的方程
  \begin{equation}
      \frac{\partial L(\Vt)}{\partial\Vt}\bigg|_{\Vt^*}=\vec{0}
  \end{equation}
  可以得到最优参数的解析解
  \begin{equation}
      (\MA^T\MA+{\color{red}\lambda\MI})\Vt=\MA^T\Vy
  \end{equation}

  $\lambda\MI$抬高了$\MA^T\MA$矩阵对角线上的值，看起来像\emph{山脊}（Ridge）
\end{frame}

\begin{frame}
  \frametitle{练习}
  使用大小不同的$\lambda$拟合数据，总结规律。
\end{frame}

\begin{frame}
  \frametitle{Logistic回归的损失函数}
  基于对概率比较的分析，Logistic回归的函数定义如下。对\emph{某个}数据点$(x_i, y_i)$
  \begin{equation}\label{eq:logistic_loss_seperated}
    l_i(\Vt)=\left\{\begin{array}{ll}
      -\log(\hat{y}_i) & \text{如果}y_i=1\\
      -\log(1-\hat{y}_i) & \text{如果}y_i=0
    \end{array}\right.
  \end{equation}
  这个定义有以下特点
  \begin{itemize}
    \item 当$y_i=1$时数据点属于$1$类，此时如果预测值$\hat{y}_i=1$则完美符合，没有损失
    \item 当$y_i=0$时数据点属于$0$类，此时如果预测值$\hat{y}_i=0$则完美符合，没有损失
    \item 损失$l\ge0$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Logistic回归的损失函数}
  公式\ref{eq:logistic_loss_seperated}是分段函数，使用不便。我们利用一个\emph{小技巧}把它们组合起来
  \begin{equation}\label{eq:logistic_loss_combined}
    l_i(\Vt)=-[y_i\cdot\log(\hat{y}_i)+(1-y_i)\cdot\log(1-\hat{y_i})]
  \end{equation}

  可以证明公式\ref{eq:logistic_loss_combined}和\ref{eq:logistic_loss_seperated}等价（试一下）

\end{frame}

\begin{frame}
  \frametitle{Logistic回归的损失函数}
  考虑所有$m$个数据点，总损失函数为
  \begin{equation*}
    \begin{split}
    L(\Vt)&=\frac{1}{m}\sum_{i=1}^m l_i(\Vt)\\
    &=-\frac{1}{m}\sum_{i=1}^m[y_i\cdot\log(\hat{y}_i)+(1-y_i)\cdot\log(1-\hat{y_i})]
    \end{split}
  \end{equation*}
  进一步整理成矩阵形式有
  \begin{equation}\label{eq:logistic_loss}
    \boxed{-\frac{1}{m}[\Vy^T\cdot\log(\sigma(\MA\Vt))+(1-\Vy)^T\cdot\log(\vec{1}-\sigma(\MA\Vt))]}
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Logsitic回归的梯度}
  对公式\ref{eq:logistic_loss}求偏导，得到Logistic回归的梯度为
  \begin{equation}
    \begin{split}
    \nabla_{\Vt}L(\Vt)&=\frac{1}{m}\MA^T(\sigma(\MA\Vt)-\Vy)\\
    &=\frac{1}{m}\MA^T(\hat{\Vy}-\Vy)
    \end{split}
  \end{equation}
  与线性回归的梯度公式\ref{eq:gradient_ls}比较，两者形式\emph{高度相似}

\end{frame}

\begin{frame}
  \frametitle{练习}
  梯度下降法进行二个属性数据的分类，画出属性空间图。注意边界
  
\end{frame}

\begin{frame}
  \frametitle{从两类到多类}
  Logistic回归只能解决分两类的问题，扩展到多类上称为Softmax模型。

  假设存在$k$个类别，每个类别都有一组\emph{打分系数}$\Vt_j,1\leq j\leq k$。
  某组样本$(\Vx_i, y_i)$与类别吻合的分数分别为
  \begin{equation}
    s_{ij}=\Vt_j^T\cdot\Vx_i
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{分数到概率}
  根据分数，最终样本$i$属于类别$j$的概率为
  \begin{equation}
    \hat{p}_{ij}=\frac{e^{s_{ij}}}{\sum_{j=1}^k e^{s_{ij}}}
  \end{equation}

  这个函数称为softmax，相对于“hard” max。
  \begin{itemize}
    \item “hard” max从一组数中挑出最大的数字，让他的对应权重为$1$，其余为$0$
    \item Softmax扩大最大数字的权重，如果\emph{反复应用}，最大元素的权重趋于$1$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Logistic回归是Softmax的特例}
  当类别数量$k=2$时，Softmax和Logistic回归等价。

  TODO证明
\end{frame}

\begin{frame}
  \frametitle{Softmax的损失函数}
  使用类似于Logistic回归的分析方法，可以构造出Softmax的损失函数
  \begin{equation}
    L(\Vt)=-\frac{1}{m}\sum_{i=1}^m\sum_{j=i}^k y_{ij}\log(\hat{p}_{ij})
  \end{equation}
  其中$y_{ij}$表示第$i$个样本是否属于第$j$类，在$\{0,1\}$内取值。
  这个损失函数又称为\emph{交叉熵}函数。
\end{frame}

\begin{frame}
  \frametitle{交叉熵}
  \emph{熵}(entropy)和\emph{交叉熵}(cross entropy)都是来自于信息论的概念。其中熵的定义
  是，对于分布函数$p(x)$，熵表示包含信息的多少，定义为
  \begin{equation}
    E(p)=-\sum_x p(x)\log({\color{red}p(x)})
  \end{equation}
  相应的交叉熵用于比较两个不同分布$p(x)$，$q(x)$差别大小，定义为
  \begin{equation}
    H(p,q)=-\sum_x p(x)\log({\color{red}q(x)})
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{交叉熵}
  $\{y_{ij}:1\leq j\leq k\}$和$\{\hat{y}_{ij}:1\leq j\leq k\}$是两个不同的分布，
  因此Softmax的损失函数符合交叉熵的定义。

  

\end{frame}
\end{document}