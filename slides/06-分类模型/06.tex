%\documentclass[14pt,aspectratio=169]{beamer}
\documentclass[14pt]{beamer}
\input{../preamble.tex}

\title{第6讲：Logistic回归}
\subtitle{线性回归的数值解}
\author{熊耀华}
\institute{交通工程系}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
  \frametitle{分类模型}
  前面的回归模型中，我们建立了函数关系$f:\Vx\to\Vy$用于从$\Vx$预测$\Vy$，此时$\Vy\in \SR$可以取任何值。

  现在我们希望建立函数，判断$\Vx$\emph{是否}满足某种条件。此时$\Vy$应当表示满足条件的概率，$\Vy\in[0,1]$

  为此我们可以构造一个函数$g:\SR\to[0,1]$，与$f$组成复合函数
  \begin{equation}
    g\circ f:\Vx\to[0,1]
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Logistic函数}
  满足$g:\SR\to[0,1]$的函数中，最常用的称为Logisitic函数（或者Sigmoid函数）
  \begin{equation}
    \sigma(t)=\frac{1}{1+e^{-t}}
  \end{equation}
  从函数图像可以看出
  \begin{itemize}
    \item $\lim_{t\to\infty}\sigma(t)=1$
    \item $\lim_{t\to-\infty}\sigma(t)=0$
    \item 关于$(0, 0.5)$对称
  \end{itemize}
  

\end{frame}
\begin{frame}
  \frametitle{Logistic回归}
  用以下复合函数拟合$\Vx$，$\Vy$
  \begin{equation}
    \sigma(\MA\Vt)
  \end{equation}
  其中，$\Vy$矢量中的元素只能取$\{0,1\}$，分别表示两种\emph{可能的类别}；$\MA=\begin{bmatrix}
    \Vx & \Vi
  \end{bmatrix}$

  通过选取最佳的参数$\Vt^*$使\emph{预测值}$\hat{\Vy}=\sigma(\MA\Vt^*)$与\emph{观测值}$\Vy$\emph{最接近}。如何定义“接近”？
\end{frame}

\begin{frame}
  \frametitle{Logistic函数的导数}
  梯度下降法求解时需要Logistic函数的导数。Logistic函数的导数具有\emph{特别形式}
  \begin{equation}
    \sigma'(t)=\sigma(t)(1-\sigma(t))
  \end{equation}
  请验证？
  
  提示：用求导规则
  $$\left(\frac{1}{u}\right)'=\frac{u'}{u^2}$$
\end{frame}

\begin{frame}
  \frametitle{如何比较概率？}
  线性回归中将接近定义为\emph{残差的平方和}，这个定义不适合Logistic回归。因为Logistic回归中的$\Vy$表示\emph{概率}，而概率的接近程度并不是线性的。
  
  考虑以下三种情况，交通事故发生的概率为$\{0.01, 0.001, 0.0001\}$。概率数字变化不大，但表示的情况差别很明显，三种情况的安全性各差了一个\emph{数量级}。

  要体现数量级的差距，用概率的\emph{对数函数}更合适。例如$\{\log0.01, \log0.001, \log0.0001\}$三个值之间距离相等。
\end{frame}

\begin{frame}
  \frametitle{Logistic回归的损失函数}
  基于对概率比较的分析，Logistic回归的函数定义如下。对\emph{某个}数据点$(x_i, y_i)$
  \begin{equation}\label{eq:logistic_loss_seperated}
    l_i(\Vt)=\left\{\begin{array}{ll}
      -\log(\hat{y}_i) & \text{如果}y_i=1\\
      -\log(1-\hat{y}_i) & \text{如果}y_i=0
    \end{array}\right.
  \end{equation}
  这个定义有以下特点
  \begin{itemize}
    \item 当$y_i=1$时数据点属于$1$类，此时如果预测值$\hat{y}_i=1$则完美符合，没有损失
    \item 当$y_i=0$时数据点属于$0$类，此时如果预测值$\hat{y}_i=0$则完美符合，没有损失
    \item 损失$l\ge0$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Logistic回归的损失函数}
  公式\ref{eq:logistic_loss_seperated}是分段函数，使用不便。我们利用一个\emph{小技巧}把它们组合起来
  \begin{equation}\label{eq:logistic_loss_combined}
    l_i(\Vt)=-[y_i\cdot\log(\hat{y}_i)+(1-y_i)\cdot\log(1-\hat{y_i})]
  \end{equation}

  可以证明公式\ref{eq:logistic_loss_combined}和\ref{eq:logistic_loss_seperated}等价（试一下）

\end{frame}

\begin{frame}
  \frametitle{Logistic回归的损失函数}
  考虑所有$m$个数据点，总损失函数为
  \begin{equation*}
    \begin{split}
    L(\Vt)&=\frac{1}{m}\sum_{i=1}^m l_i(\Vt)\\
    &=-\frac{1}{m}\sum_{i=1}^m[y_i\cdot\log(\hat{y}_i)+(1-y_i)\cdot\log(1-\hat{y_i})]
    \end{split}
  \end{equation*}
  进一步整理成矩阵形式有
  \begin{equation}\label{eq:logistic_loss}
    \boxed{-\frac{1}{m}[\Vy^T\cdot\log(\sigma(\MA\Vt))+(1-\Vy)^T\cdot\log(\Vi-\sigma(\MA\Vt))]}
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Logsitic回归的梯度}
  对公式\ref{eq:logistic_loss}求偏导，得到Logistic回归的梯度为
  \begin{equation}
    \begin{split}
    \nabla_{\Vt}L(\Vt)&=\frac{1}{m}\MA^T(\sigma(\MA\Vt)-\Vy)\\
    &=\frac{1}{m}\MA^T(\hat{\Vy}-\Vy)
    \end{split}
  \end{equation}
  与线性回归的梯度公式\ref{eq:gradient_ls}比较，两者形式\emph{高度相似}

\end{frame}

\begin{frame}
  \frametitle{练习}
  梯度下降法进行二个属性数据的分类，画出属性空间图。注意边界
  
\end{frame}

\begin{frame}
  \frametitle{从两类到多类}
  Logistic回归只能解决分两类的问题，扩展到多类上称为Softmax模型。

  假设存在$k$个类别，每个类别都有一组\emph{打分系数}$\Vt_j,1\leq j\leq k$。
  某组样本$(\Vx_i, y_i)$与类别吻合的分数分别为
  \begin{equation}
    s_{ij}=\Vt_j^T\cdot\Vx_i
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{分数到概率}
  根据分数，最终样本$i$属于类别$j$的概率为
  \begin{equation}
    \hat{p}_{ij}=\frac{e^{s_{ij}}}{\sum_{j=1}^k e^{s_{ij}}}
  \end{equation}

  这个函数称为softmax，相对于“hard” max。
  \begin{itemize}
    \item “hard” max从一组数中挑出最大的数字，让他的对应权重为$1$，其余为$0$
    \item Softmax扩大最大数字的权重，如果\emph{反复应用}，最大元素的权重趋于$1$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Logistic回归是Softmax的特例}
  当类别数量$k=2$时，Softmax和Logistic回归等价。

  TODO证明
\end{frame}

\begin{frame}
  \frametitle{Softmax的损失函数}
  使用类似于Logistic回归的分析方法，可以构造出Softmax的损失函数
  \begin{equation}
    L(\Vt)=-\frac{1}{m}\sum_{i=1}^m\sum_{j=i}^k y_{ij}\log(\hat{p}_{ij})
  \end{equation}
  其中$y_{ij}$表示第$i$个样本是否属于第$j$类，在$\{0,1\}$内取值。
  这个损失函数又称为\emph{交叉熵}函数。
\end{frame}

\begin{frame}
  \frametitle{交叉熵}
  \emph{熵}(entropy)和\emph{交叉熵}(cross entropy)都是来自于信息论的概念。其中熵的定义
  是，对于分布函数$p(x)$，熵表示包含信息的多少，定义为
  \begin{equation}
    E(p)=-\sum_x p(x)\log({\color{red}p(x)})
  \end{equation}
  相应的交叉熵用于比较两个不同分布$p(x)$，$q(x)$差别大小，定义为
  \begin{equation}
    H(p,q)=-\sum_x p(x)\log({\color{red}q(x)})
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{交叉熵}
  $\{y_{ij}:1\leq j\leq k\}$和$\{\hat{y}_{ij}:1\leq j\leq k\}$是两个不同的分布，
  因此Softmax的损失函数符合交叉熵的定义。

  

\end{frame}
\end{document}